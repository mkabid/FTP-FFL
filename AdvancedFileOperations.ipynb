{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb3de4ac-bbe5-4e71-a514-b7d7e5afd856",
   "metadata": {},
   "source": [
    "#  Advanced File operations\n",
    "\n",
    "### Lesson Objectives\n",
    "- Merge and concatenate multiple files\n",
    "\n",
    "- Split large files into smaller segments\n",
    "\n",
    "- Convert between different file formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d378d0a-be33-4158-b291-bf754a144c61",
   "metadata": {},
   "source": [
    "## Merging and Concatenating Multiple Files\n",
    "Concatenating Files is an essential task in data processing, particularly when integrating data from different sources. It plays a critical role in the  **data pre-processing** phase of the KDD (Knowledge Discovery in Databases) process, where preparing clean and unified datasets is a prerequisite for meaningful analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe65b874-8b0d-4463-a994-0b187ba67847",
   "metadata": {},
   "source": [
    "## Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0d475ce2-780a-441d-8b7e-04fe53dc0443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, world !\n",
      "this is the content of the file 1\n",
      "this is the content of the file 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "directory=r\"C:\\Users\\merie\\Python-part3\\FileHandling\\\\\"\n",
    "path_file1=directory+\"file1.txt\" \n",
    "path_file2=directory+\"file2.txt\" \n",
    "path_file3=directory+\"file3.txt\" \n",
    "with open(path_file3, 'w',encoding='utf-8') as f3:\n",
    "    with open(path_file1, 'r',encoding='utf-8') as f1:\n",
    "        f3.write(f1.read())\n",
    "        f3.write(\"\\n\")\n",
    "    with open(path_file2, 'r',encoding='utf-8') as f2:\n",
    "        f3.write(f2.read())\n",
    "with open(path_file3, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3072be27-dcc5-44ba-8c56-bb523e0c5fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3dfbc5-e40e-49af-b838-0a45a0ad168f",
   "metadata": {},
   "source": [
    "## CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8a83436-0807-4431-8a33-935dae419228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name,age,city\n",
      "\n",
      "Amira,20,Oran\n",
      "\n",
      "\n",
      "Ahmed,10,Mascara\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "# CSV Files Creation\n",
    "# File 1\n",
    "with open('file1.csv', 'w') as file:\n",
    "    writer=csv.writer(file)\n",
    "    writer.writerow([\"name\",\"age\",\"city\"]) # writing header\n",
    "    writer.writerow([\"Amira\",20, \"Oran\"])\n",
    "# File 2\n",
    "with open('file2.csv', 'w') as file:\n",
    "    writer=csv.writer(file)\n",
    "    writer.writerow([\"name\",\"age\",\"city\"]) # writing header\n",
    "    writer.writerow([\"Ahmed\",10, \"Mascara\"])\n",
    "\n",
    "# Files Merging\n",
    "with open(\"merged_file.csv\", \"w\",newline='') as f3:\n",
    "    writer = csv.writer(f3)\n",
    "    with open(\"file1.csv\",\"r\") as f1:\n",
    "        content1=csv.reader(f1)\n",
    "        header=next(content1)\n",
    "        writer.writerow(header) #copy the header\n",
    "        for row in content1:\n",
    "            writer.writerow(row)\n",
    "    with open(\"file2.csv\",\"r\") as f2:\n",
    "        content2=csv.reader(f2)\n",
    "        next(content2) #skip the header\n",
    "        for row in content2:\n",
    "            writer.writerow(row)\n",
    "    #reading line by line\n",
    "with open(\"merged_file.csv\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9202d264-b5c6-462f-9cbe-cdc84e54522e",
   "metadata": {},
   "source": [
    "## XLSX Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6c7d1ee4-5498-4ee9-bb4b-123afeb68dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import pandas as pd\n",
    "#create file1.xlsx\n",
    "workbook = openpyxl.Workbook()\n",
    "\n",
    "# Get the active sheet\n",
    "sheet = workbook.active\n",
    "sheet.title = \"Person\"  # Rename the sheet\n",
    "\n",
    "# Write headers\n",
    "sheet['A1'] = \"Name\"\n",
    "sheet['B1'] = \"Age\" \n",
    "\n",
    "# Write student data\n",
    "sheet['A2'] = \"Sonia\"\n",
    "sheet['B2'] = \"19\" \n",
    "\n",
    "# Save the file\n",
    "workbook.save('file1.xlsx')\n",
    "\n",
    "#create file2.xlsx\n",
    "workbook = openpyxl.Workbook()\n",
    "\n",
    "# Get the active sheet\n",
    "sheet = workbook.active\n",
    "sheet.title = \"Person\"  # Rename the sheet\n",
    "\n",
    "# Write headers\n",
    "sheet['A1'] = \"Name\"\n",
    "sheet['B1'] = \"Age\" \n",
    "\n",
    "# Write student data\n",
    "sheet['A2'] = \"Karim\"\n",
    "sheet['B2'] = \"29\" \n",
    "\n",
    "# Save the file\n",
    "workbook.save('file2.xlsx')\n",
    "\n",
    "#Merging file1 and file 2\n",
    "\n",
    "#Read the two files\n",
    "df1=pd.read_excel(\"file1.xlsx\")\n",
    "df2=pd.read_excel(\"file2.xlsx\")\n",
    "\n",
    "#concatenate\n",
    "merged_df=pd.concat([df1,df2],ignore_index=True)\n",
    "merged_df.to_excel(\"file3.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cbd69b-75c0-4ac1-b214-d4f95573da59",
   "metadata": {},
   "source": [
    "## JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1abe09f7-7f4a-4be3-aef2-ab68e0ec3faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'nom': 'Amina', 'age': 25}, {'nom': 'Malik', 'age': 35}]\n"
     ]
    }
   ],
   "source": [
    "# JSON files creation\n",
    "info_person1 =[{\n",
    "    'nom':\"Amina\",\n",
    "    'age': 25\n",
    "}]\n",
    "info_person2 =[{\n",
    "    'nom':\"Malik\",\n",
    "    'age': 35\n",
    "}]\n",
    "with open(\"file1.json\",\"w\") as f1:\n",
    "    json.dump(info_person1,f1,indent=4)\n",
    "\n",
    "with open(\"file2.json\",\"w\") as f2:\n",
    "    json.dump(info_person2,f2,indent=4)\n",
    "\n",
    "# Data Loading\n",
    "\n",
    "with open(\"file1.json\",\"r\") as f1:\n",
    "    data1=json.load(f1)\n",
    "with open(\"file2.json\",\"r\") as f2:\n",
    "    data2=json.load(f2)\n",
    "\n",
    "# File Merging\n",
    "\n",
    "with open(\"file3.json\", \"w\") as f3:\n",
    "    json.dump(data1+data2,f3,indent=4)\n",
    "\n",
    "with open(\"file3.json\", \"r\") as f3:\n",
    "    data=json.load(f3)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e2051-b378-4379-9fe1-da2f8d1df2d6",
   "metadata": {},
   "source": [
    "## XML Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "53a85b38-7f5f-4fea-b3a9-09453e40edbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "#create XML files\n",
    "# Step 1: Create root element\n",
    "person1 = ET.Element(\"person\")\n",
    "\n",
    "# Step 2: Add child elements\n",
    "name = ET.SubElement(person1, \"name\")\n",
    "name.text = \"Amel\"\n",
    "\n",
    "age = ET.SubElement(person1, \"age\")\n",
    "age.text = \"25\"\n",
    "\n",
    "# Step 3: Create ElementTree and save\n",
    "tree = ET.ElementTree(person1)\n",
    "tree.write(\"file1.xml\")\n",
    "\n",
    "person2 = ET.Element(\"person\")\n",
    "\n",
    "# Step 2: Add child elements\n",
    "name = ET.SubElement(person2, \"name\")\n",
    "name.text = \"Latif\"\n",
    "\n",
    "age = ET.SubElement(person2, \"age\")\n",
    "age.text = \"25\"\n",
    "\n",
    "# Step 3: Create ElementTree and save\n",
    "tree = ET.ElementTree(person2)\n",
    "tree.write(\"file2.xml\")\n",
    "\n",
    "#parsing\n",
    "#Create a new root and append both root elements directly\n",
    "people_root = ET.Element(\"people\")\n",
    "\n",
    "tree1 = ET.parse('file1.xml')\n",
    "tree2 = ET.parse('file2.xml')\n",
    "#append the root elements directly \n",
    "people_root.append(tree1.getroot())\n",
    "people_root.append(tree2.getroot())\n",
    "\n",
    "# Save\n",
    "merged_tree = ET.ElementTree(people_root)\n",
    "merged_tree.write(\"file3.xml\", encoding=\"utf-8\", xml_declaration=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b02dfe-3c63-4b26-b936-425abbe81b8e",
   "metadata": {},
   "source": [
    "## YAML Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1e80c1b4-689b-470c-8b6e-f421633f518c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- age: 25\n",
      "  name: Alice\n",
      "- age: 45\n",
      "  name: Omar\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "#file creation\n",
    "person_data1 = [{\n",
    "    \"name\": \"Alice\",\n",
    "    \"age\": 25\n",
    "}]\n",
    "\n",
    "# Write to YAML file\n",
    "with open('file1.yaml', 'w') as file:\n",
    "    yaml.dump(person_data1, file)\n",
    "\n",
    "person_data2 = [{\n",
    "    \"name\": \"Omar\",\n",
    "    \"age\": 45\n",
    "}]\n",
    "# Write to YAML file\n",
    "with open('file2.yaml', 'w') as file:\n",
    "    yaml.dump(person_data2, file)\n",
    "\n",
    "#load the content of both yaml files\n",
    "with open(\"file1.yaml\",\"r\") as f1, open(\"file2.yaml\",\"r\") as f2:\n",
    "    data1=yaml.safe_load(f1)\n",
    "    data2=yaml.safe_load(f2)\n",
    "with open(\"file3.xml\",\"w\") as f3:\n",
    "    yaml.dump(data1+data2,f3)\n",
    "with open(\"file3.xml\",\"r\") as f3:\n",
    "    print(f3.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d89ce17-ccb0-42a7-93fe-855f62293509",
   "metadata": {},
   "source": [
    "## Spliting and Segmenting a file \n",
    "- The segmentation helps to simplify data by breaking it into smaller, more manageable parts. It improves efficiency in storage and processing while making the data easier to analyze, visualize, or distribute for specific tasks. To effectively split and segment files, it is essential to first define the splitting criteria: this can be based on the number of lines or rows (e.g., every 1000 lines), file size (e.g., a maximum of 5 MB per file), specific key or column values (such as category or date), or custom logic (like time ranges or section markers).\n",
    "- The splitting is done by opening the original file in read mode, then reading its contents and dividing them based on a specific condition (such as line count, character count, or a delimiter), and finally writing each part into separate output files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2631262-9693-4b88-90e5-deae112b70b5",
   "metadata": {},
   "source": [
    "### Text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4a52b8bd-5322-4794-bd32-a11d728237b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = \"Splitting\\\\\"\n",
    "\n",
    "# Split TXT\n",
    "with open(Path+\"file.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    part1 = lines[:5]\n",
    "    part2 = lines[5:]\n",
    "\n",
    "with open(Path+\"students_part1.txt\", \"w\", encoding=\"utf-8\") as f1:\n",
    "    f1.writelines(part1)\n",
    "with open(Path+\"students_part2.txt\", \"w\", encoding=\"utf-8\") as f2:\n",
    "    f2.writelines(part2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da25230-6aba-4e85-8284-12c3fa873462",
   "metadata": {},
   "source": [
    "### CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f39cade5-9ab2-4fd0-8037-8a2a3170de30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Split CSV\n",
    "Path = \"Splitting\\\\\"\n",
    "\n",
    "with open(Path+\"file.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    rows = list(csv.reader(f))\n",
    "    header, data = rows[0], rows[1:]\n",
    "    part1, part2 = data[:5], data[5:]\n",
    "\n",
    "for idx, part in enumerate([part1, part2], 1):\n",
    "    with open(Path+f\"students_part{idx}.csv\", \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(part)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb77e4f-bc5b-4ead-9ec3-c547a0c1295b",
   "metadata": {},
   "source": [
    "### XLSX Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8165e64f-8e5f-4bf5-b00a-48f66f891126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "input_path = \"Splitting/file.xlsx\"\n",
    "output_path1 = \"Splitting/file_part1.xlsx\"\n",
    "output_path2 = \"Splitting/file_part2.xlsx\"\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(input_path)\n",
    "\n",
    "# Calculate midpoint to split\n",
    "mid = len(df) // 2\n",
    "\n",
    "# Split the DataFrame into two parts\n",
    "df_part1 = df.iloc[:mid]\n",
    "df_part2 = df.iloc[mid:]\n",
    "\n",
    "# Save each part to a new Excel file\n",
    "df_part1.to_excel(output_path1, index=False)\n",
    "df_part2.to_excel(output_path2, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311235e2-034a-4b2c-a7dc-f54d4d5df0a3",
   "metadata": {},
   "source": [
    "### JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b9964dfa-42cc-4dea-8018-14e8f8d11d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "Path = \"Splitting\\\\\"\n",
    "\n",
    "if not os.path.exists(Path+\"file.json\"):\n",
    "    students = [\n",
    "        {\"Name\": \"Ali\", \"Age\": 20},\n",
    "        {\"Name\": \"Sara\", \"Age\": 22},\n",
    "        {\"Name\": \"Omar\", \"Age\": 23}\n",
    "    ]\n",
    "with open(Path+\"file.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(students, f, indent=4)\n",
    "\n",
    "# Split JSON file into tow files\n",
    "with open(Path+\"file.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    json1 = data[:5]\n",
    "    json2 = data[5:]\n",
    "\n",
    "with open(Path+\"students_part1.json\", \"w\", encoding=\"utf-8\") as f1:\n",
    "    json.dump(json1, f1, indent=2)\n",
    "with open(Path+\"students_part2.json\", \"w\", encoding=\"utf-8\") as f2:\n",
    "    json.dump(json2, f2, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f719a2-6c70-4522-ae15-05ffc09430c2",
   "metadata": {},
   "source": [
    "### XML Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1ae4e4e2-578e-4262-a03a-2ac8a2493869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total students found: 10\n"
     ]
    }
   ],
   "source": [
    "# Create root element\n",
    "students_root = ET.Element(\"students\")\n",
    "\n",
    "# Add 10 students\n",
    "students_data = [\n",
    "    (\"Ikram\", \"20\"), (\"Mohammed\", \"22\"), (\"Carlos\", \"19\"), (\"Dalila\", \"21\"), \n",
    "    (\"Ismael\", \"23\"), (\"Farah\", \"20\"), (\"Djamil\", \"24\"), (\"Hanane\", \"22\"),\n",
    "    (\"Mehdi\", \"25\"), (\"Adel\", \"21\")\n",
    "]\n",
    "\n",
    "for name, age in students_data:\n",
    "    student = ET.SubElement(students_root, \"student\")\n",
    "    name_elem = ET.SubElement(student, \"name\")\n",
    "    name_elem.text = name\n",
    "    age_elem = ET.SubElement(student, \"age\")\n",
    "    age_elem.text = age\n",
    "\n",
    "# Create ElementTree and save\n",
    "tree = ET.ElementTree(students_root)\n",
    "tree.write(Path + \"file.xml\", encoding=\"utf-8\", xml_declaration=True)\n",
    "\n",
    "# Parse the original XML file\n",
    "tree = ET.parse(Path + \"file.xml\")\n",
    "root = tree.getroot()\n",
    "\n",
    "# Get all student elements\n",
    "all_students = list(root)  # This gets all <student> elements\n",
    "\n",
    "print(f\"Total students found: {len(all_students)}\")\n",
    "\n",
    "# Create first part (students 1-5)\n",
    "root_part1 = ET.Element(\"students\")  # Create new root for part 1\n",
    "for student in all_students[:5]:     # Take first 5 students\n",
    "    root_part1.append(student)       # Add them to new root\n",
    "\n",
    "# Save first part\n",
    "tree_part1 = ET.ElementTree(root_part1)\n",
    "tree_part1.write(Path + \"students_part1.xml\", encoding=\"utf-8\", xml_declaration=True)\n",
    "\n",
    "\n",
    "# Create second part (students 6-10)  \n",
    "root_part2 = ET.Element(\"students\")  # Create new root for part 2\n",
    "for student in all_students[5:]:     # Take remaining 5 students\n",
    "    root_part2.append(student)       # Add them to new root\n",
    "\n",
    "# Save second part\n",
    "tree_part2 = ET.ElementTree(root_part2)\n",
    "tree_part2.write(Path + \"students_part2.xml\", encoding=\"utf-8\", xml_declaration=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac279bb-07f2-4dc0-b9e5-c86fc50e305f",
   "metadata": {},
   "source": [
    "### YAML Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fdce5f9c-1d5c-4c9d-aa9e-3f742ee1a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split YAML file into tow files\n",
    "# Create data for 10 students\n",
    "students_data = [\n",
    "    {\"name\": \"Alice\", \"age\": 20},\n",
    "    {\"name\": \"Bob\", \"age\": 22},\n",
    "    {\"name\": \"Carlos\", \"age\": 19},\n",
    "    {\"name\": \"Diana\", \"age\": 21},\n",
    "    {\"name\": \"Ethan\", \"age\": 23},\n",
    "    {\"name\": \"Fiona\", \"age\": 20},\n",
    "    {\"name\": \"George\", \"age\": 24},\n",
    "    {\"name\": \"Hannah\", \"age\": 22},\n",
    "    {\"name\": \"Ivan\", \"age\": 25},\n",
    "    {\"name\": \"Julia\", \"age\": 21}\n",
    "]\n",
    "\n",
    "# Create the original YAML file\n",
    "with open(Path + \"file.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.dump(students_data, f, allow_unicode=True)\n",
    "    \n",
    "with open(Path+\"file.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = yaml.safe_load(f)\n",
    "    yaml1 = data[:5]\n",
    "    yaml2 = data[5:]\n",
    "\n",
    "with open(Path+\"students_part1.yaml\", \"w\", encoding=\"utf-8\") as f1:\n",
    "    yaml.dump(yaml1, f1, allow_unicode=True)\n",
    "with open(Path+\"students_part2.yaml\", \"w\", encoding=\"utf-8\") as f2:\n",
    "    yaml.dump(yaml2, f2, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3f5870-c18e-400d-a7ce-4e779cf7c01a",
   "metadata": {},
   "source": [
    "## File Format Conversion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bb004f-78dd-4f59-950b-d768261c342d",
   "metadata": {},
   "source": [
    "### Convert TXT file to CV, XLSX, JSON, XML, and YAML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bcd47351-ddef-4097-ab9e-1809e89007ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import yaml\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read and parse the text file\n",
    "students = [] #this wil contain a list of student dictionaries\n",
    "with open(\"Converting\\\\file.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(\", \")\n",
    "        student = {kv.split(\": \")[0]: kv.split(\": \")[1] for kv in parts} # obtain a student dictionnary by extracting key and value \n",
    "        students.append(student)\n",
    "\n",
    "# Step 2: Save to CSV\n",
    "with open(\"Converting\\\\file.csv\", \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"name\", \"age\", \"city\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(students)\n",
    "\n",
    "# Step 3: Save to XLSX\n",
    "df = pd.DataFrame(students)\n",
    "df.to_excel(\"Converting\\\\file.xlsx\", index=False)\n",
    "\n",
    "# Step 4: Save to JSON\n",
    "with open(\"Converting\\\\file.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(students, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Step 5: Save to XML\n",
    "root = ET.Element(\"Sudents\")\n",
    "for s in students:\n",
    "    student_elem = ET.SubElement(root, \"student\")\n",
    "    for key, val in s.items():\n",
    "        ET.SubElement(student_elem, key).text = val\n",
    "tree = ET.ElementTree(root)\n",
    "tree.write(\"Converting\\\\file.xml\", encoding=\"utf-8\", xml_declaration=True)\n",
    "\n",
    "# Step 6: Save to YAML\n",
    "with open(\"Converting\\\\file.yaml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.dump(students, f, allow_unicode=True, sort_keys=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05dc9f6-9992-40e2-a2c6-35a814caadcc",
   "metadata": {},
   "source": [
    "# Let's practice \n",
    "## Exercise 1:\n",
    "Exercise 1: File Handling in Python \n",
    "\n",
    "1.\tWrite a program that:\n",
    "-\tCreates a text file called students.txt containing 5 student names.\n",
    "-\tCreates a binary file grades.bin with random integers (0–20).\n",
    "-\tCreates a CSV file students.csv with two columns: Name, Age.\n",
    "-\tCreates a JSON file students.json with a dictionary of student details.\n",
    "-\tCreates an Excel file students.xlsx with the same data.\n",
    "-\tCreates a YAML file students.yaml with at least 3 student profiles.\n",
    "\n",
    "2.\tAdd one new student record to:\n",
    "-\tstudents.txt (append mode).\n",
    "-\tstudents.csv (append a row).\n",
    "-\tstudents.json (add a new key–value).\n",
    "-\tstudents.xlsx (add a new row using openpyxl).\n",
    "-\tstudents.yaml (add a new mapping).\n",
    "\n",
    "3.\tDeleting Files\n",
    "-\tWrite a Python script that deletes: grades.bin and students.yaml.\n",
    "-\tUse exception handling to check if the file exists before deleting.\n",
    "\n",
    "4.\tWrite a program that merges:\n",
    "-\tMultiple text files (students_part1.txt, students_part2.txt) into one.\n",
    "-\tMultiple CSV files into merged_students.csv.\n",
    "-\tMultiple JSON files into one big JSON object.\n",
    "-\tMultiple Excel files into a single sheet (all_students.xlsx).\n",
    "\n",
    "5.\tWrite a program that:\n",
    "-\tSplits students.txt into two smaller files (first half, second half).\n",
    "-\tSplits students.csv into multiple files, each containing only 2 rows.\n",
    "-\tSplits a JSON array into chunks of size 2.\n",
    "-\tSplits an Excel file into separate sheets, one per student\n",
    "\n",
    "6.\tWrite a script that converts:\n",
    "-\tstudents.csv to students.json\n",
    "-\tstudents.json to students.yaml\n",
    "-\tstudents.xlsx to students.csv\n",
    "-\tstudents.yaml to students.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c060f0-6b45-4eb0-866d-3e002f00f1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## you solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c71375e-0796-4815-86ba-3a9872e76214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
